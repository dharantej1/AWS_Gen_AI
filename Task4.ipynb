{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Conversational Interface - Chat with Llama 3 and Titan Premier LLMs\n",
    "\n",
    "In this notebook, you build a chatbot using the llama3-8b-instruct and titan-text-premier Foundation Models (FMs) in Amazon Bedrock.\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can enhance the user experience for your customers. Chatbots use natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. You can use chatbots in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. Users can access them through various channels such as websites, social media platforms, and messaging apps.\n",
    "\n",
    "- **Chatbot (Basic)**, Zero Shot chatbot with a FM model\n",
    "- **Chatbot using prompt**, template(LangChain) - Chatbot with some context provided in the prompt template\n",
    "- **Chatbot with persona**, Chatbot with defined roles. i.e. Career Coach and Human interactions\n",
    "- **Contextual-aware chatbot**, Passing in context through an external file by generating embeddings.\n",
    "\n",
    "## LangChain framework for building Chatbot with Amazon Bedrock\n",
    "\n",
    "In conversational interfaces such as chatbots, remembering previous interactions becomes highly important, both at a short-term and long-term level.\n",
    "\n",
    "The LangChain framework provides memory components in two forms. First, LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular. Secondly, LangChain provides easy ways to incorporate these utilities into chains, allowing you to easily define and interact with different types of abstractions, which make powerful chatbots easy for you to build.\n",
    "\n",
    "## Building a Chatbot with Context - Key Elements\n",
    "\n",
    "The first process in building a context-aware chatbot is to generate embeddings for the context. Typically, you have an ingestion process which runs through your embedding model and generates the embeddings, which will then be stored in a vector store. In this notebook, you use the Titan Embeddings model for this. The second process is user request orchestration, interaction, invoking, and returning the results. This involves orchestrating the user request, interacting with the necessary models/components to gather information, invoking the chatbot to formulate a response, and then returning the chatbot's response back to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1: Environment setup\n",
    "\n",
    "In this task, you set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ignore warnings and create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime',region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format instructions into a conversational prompt\n",
    "from typing import Dict, List\n",
    "\n",
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format instructions where conversation roles must alternate system/user/assistant/user/assistant/...\"\"\"\n",
    "    prompt: List[str] = []\n",
    "    for instruction in instructions:\n",
    "        if instruction[\"role\"] == \"system\":\n",
    "            prompt.extend([\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            prompt.extend([\"<|start_header_id|>user<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid role: {instruction['role']}. Role must be either 'user' or 'system'.\")\n",
    "    prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 4.2: Using chat history from LangChain to start the conversation\n",
    "\n",
    "In this task, you enable the chatbot to carry conversational context across multiple interactions with users. Having a conversational memory is crucial for Chatbots to hold meaningful, coherent dialogues over time.\n",
    "\n",
    "You implement conversational memory capabilities by building on top of LangChain's InMemoryChatMessageHistory class. This object stores the conversations between the user and the chatbot, and the history is available the chatbot agent so that it can leverage the context from a previous conversation.\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** The model outputs are non-deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: how are you?\n",
      "AI: I'm just a language model, I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "chat_model=ChatBedrock(\n",
    "    model_id=\"meta.llama3-8b-instruct-v1:0\" , \n",
    "    client=bedrock_client)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "query=\"how are you?\"\n",
    "response=wrapped_chain.invoke({\"input\": query})\n",
    "# Printing history to see the history being built out. \n",
    "print(history)\n",
    "# For the rest of the conversation, the output will only include response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Questions\n",
    "\n",
    "The model has responded with an initial message. Now, you ask it a few questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new garden! Exciting! Here are a few tips to help you get started:\n",
      "\n",
      "1. **Choose the right location**: Make sure the spot you've chosen gets at least 6 hours of direct sunlight a day. Also, consider the soil type and drainage. Avoid areas with standing water or where water tends to collect.\n",
      "2. **Prepare the soil**: Test the pH level of your soil and amend it if necessary. Most vegetables and flowers prefer a slightly acidic to neutral soil pH (6.0-7.0). Add organic matter like compost or manure to improve soil structure and fertility.\n",
      "3. **Plan your garden**: Decide what you want to grow and create a rough layout. Consider companion planting, where certain plants benefit from being grown together. Make sure to leave enough space between plants for proper growth and air circulation.\n",
      "4. **Start small**: Don't try to tackle too much at once. Begin with a small garden and gradually expand as you gain experience.\n",
      "5. **Use good quality seeds and seedlings**: Invest in high-quality seeds and seedlings to ensure a strong start. Research the specific growing requirements for each plant to ensure you're providing the best conditions.\n",
      "6. **Water wisely**: Water your plants deeply but infrequently to encourage deep root growth. Avoid overhead watering, which can lead to fungal diseases. Instead, use soaker hoses or water at the base of the plants.\n",
      "7. **Keep a garden journal**: Record your plantings, weather patterns, and any issues you encounter. This will help you track your progress and make informed decisions for future gardens.\n",
      "8. **Be patient**: Gardening takes time and effort. Don't get discouraged if things don't go as planned initially. Learn from your mistakes and enjoy the process.\n",
      "\n",
      "Remember, gardening is a journey, and it's okay to make mistakes. With these tips, you'll be well on your way to creating a thriving and beautiful garden!\n",
      "\n",
      "What type of garden are you planning to start?\n"
     ]
    }
   ],
   "source": [
    "#new questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"Give me a few tips on how to start a new garden.\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build on the questions\n",
    "\n",
    "Now, ask a question without mentioning the word garden to see if the model can understand the previous conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're planning to start a bug garden! That's a unique and fascinating endeavor. Here are some tips to help you get started:\n",
      "\n",
      "1. **Choose the right bugs**: Decide which types of bugs you want to attract and care for. Some popular options include:\n",
      "\t* Ladybugs (Lady beetles)\n",
      "\t* Fireflies (Lightning bugs)\n",
      "\t* Butterflies (Monarch, Swallowtail, etc.)\n",
      "\t* Bees (Honeybees, Bumblebees, etc.)\n",
      "\t* Ants (Harvester ants, etc.)\n",
      "2. **Provide the right habitat**: Create a bug-friendly environment by:\n",
      "\t* Planting native flowers, herbs, and grasses that provide nectar, pollen, and shelter\n",
      "\t* Creating a bug hotel or insect house using materials like bamboo, straws, and leaves\n",
      "\t* Providing a source of water, like a shallow dish or birdbath with fresh water and a few rocks or sticks for perching\n",
      "3. **Use the right food**: Offer a variety of foods to attract and sustain your bugs. Some options include:\n",
      "\t* Sugar water (4:1 ratio) for butterflies and bees\n",
      "\t* Fruit, like apples or bananas, for ladybugs and fireflies\n",
      "\t* Pollen-rich plants, like sunflowers or zinnias, for bees and butterflies\n",
      "\t* Insect-friendly plants, like dill or fennel, for ants and other beneficial insects\n",
      "4. **Create a bug-friendly environment**: Consider the following:\n",
      "\t* Avoid using pesticides, which can harm or kill beneficial insects\n",
      "\t* Use natural pest control methods, like neem oil or diatomaceous earth\n",
      "\t* Create a sheltered area with rocks, logs, or other hiding spots for bugs to escape harsh weather\n",
      "5. **Observe and learn**: Pay attention to the bugs that visit your garden and learn about their habits, habitats, and needs. This will help you create a more welcoming environment and attract a diverse range of insects.\n",
      "\n",
      "Remember to be patient and enjoy the process of creating a bug garden. It may take some time for the bugs to discover and inhabit your garden, but with the right conditions and care, you'll be rewarded with a thriving and fascinating ecosystem!\n",
      "\n",
      "Do you have any specific questions about creating a bug garden?\n"
     ]
    }
   ],
   "source": [
    "# build on the questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"bugs\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing this conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're welcome! It was nice chatting with you. If you have any career-related questions or concerns in the future, feel free to reach out. Otherwise, I don't know.\n"
     ]
    }
   ],
   "source": [
    "# finishing the conversation\n",
    "instructions = [{\"role\": \"user\", \"content\": \"That's all, thank you!\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.3: Chatbot using prompt template (LangChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you use the default PromptTemplate that is responsible for the construction of this input. LangChain provides several classes and functions to make constructing and working with prompts easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  prompt for a conversational agent\n",
    "def format_prompt(actor:str, input:str):\n",
    "    formatted_prompt: List[str] = []\n",
    "    if actor == \"system\":\n",
    "        prompt_template=\"\"\"<|begin_of_text|><|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    elif actor == \"user\":\n",
    "        prompt_template=\"\"\"<|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid role: {actor}. Role must be either 'user' or 'system'.\")   \n",
    "    prompt = PromptTemplate.from_template(prompt_template)     \n",
    "    formatted_prompt.extend(prompt.format(actor=actor,input=input))\n",
    "    formatted_prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat user experience\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            with self.out:\n",
    "                print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        response = self.qa.invoke({\"input\": prompt})\n",
    "                        result=response['answer']\n",
    "                    else:\n",
    "                        instructions = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                        #result = self.qa.invoke({'input': format_prompt(\"user\",prompt)}) #, 'history':chat_history})\n",
    "                        result = self.qa.invoke({\"input\": format_instructions(instructions)})\n",
    "                except:\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print(f\"AI:{result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You:\", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start a chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f071752134f745f5af053c755f93bf34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start chat\n",
    "history = InMemoryChatMessageHistory() #reset chat history\n",
    "chat = ChatUX(wrapped_chain)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What are the career options in AI?\n",
      "AI: What an exciting field! Artificial Intelligence (AI) has opened up numerous career opportunities across various industries. Here are some of the most in-demand roles:\n",
      "\n",
      "1. **Machine Learning Engineer**: Design and develop AI models, algorithms, and systems that can learn from data.\n",
      "2. **Data Scientist**: Extract insights from large datasets to inform business decisions, using AI and machine learning techniques.\n",
      "3. **AI Researcher**: Conduct research in AI and machine learning, exploring new applications and improving existing technologies.\n",
      "4. **Natural Language Processing (NLP) Engineer**: Develop AI-powered language processing systems for applications like chatbots, voice assistants, and text analysis.\n",
      "5. **Computer Vision Engineer**: Design and develop AI-powered computer vision systems for applications like image recognition, object detection, and facial recognition.\n",
      "6. **Robotics Engineer**: Develop AI-powered robots that can perform tasks autonomously, using sensors, cameras, and other technologies.\n",
      "7. **AI Ethics Specialist**: Ensure AI systems are developed and used responsibly, considering ethical implications and potential biases.\n",
      "8. **AI Developer**: Build and deploy AI-powered applications, such as predictive maintenance, supply chain optimization, and customer service chatbots.\n",
      "9. **AI Consultant**: Help organizations implement AI solutions, providing strategic guidance and technical expertise.\n",
      "10. **AI Research Scientist**: Conduct research in AI and machine learning, publishing papers and presenting at conferences.\n",
      "11. **Autonomous Systems Engineer**: Design and develop autonomous systems, such as self-driving cars, drones, and robots.\n",
      "12. **AI Product Manager**: Oversee the development and launch of AI-powered products, ensuring they meet customer needs and business goals.\n",
      "13. **AI Quality Assurance Engineer**: Test and ensure the quality of AI-powered systems, identifying and addressing defects.\n",
      "14. **AI Technical Writer**: Create documentation and training materials for AI-powered systems, helping users understand and utilize the technology.\n",
      "15. **AI Sales Representative**: Sell AI-powered solutions to organizations, highlighting the benefits and value they can bring.\n",
      "\n",
      "These are just a few examples of the many career paths available in AI. Remember to stay up-to-date with industry developments, as new roles and specializations emerge as AI continues to evolve.\n",
      "Human: How to fix my car?\n",
      "AI: I don't know. As a career coach, I'm not qualified to provide advice on car repairs. I recommend taking your car to a trusted mechanic or a repair shop for assistance.\n",
      "Human: <|start_header_id|>user<|end_header_id|>\n",
      "hello <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AI: Hello! I'm your career coach. I'm here to help you with any career-related questions or concerns you may have. What's been on your mind lately? Are you looking to make a career change, or maybe seeking advice on how to advance in your current role? Let me know and I'll do my best to assist you!\n"
     ]
    }
   ],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 4.4: Chatbot with persona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, Artificial Intelligence(AI) assistant plays the role of a career coach. You can inform the chatbot about its persona (or role) using a system message. Continue to leverage the InMemoryChatMessageHistory class to maintain conversational context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What an exciting field! Artificial Intelligence (AI) has opened up numerous career opportunities across various industries. Here are some of the most in-demand roles:\n",
      "\n",
      "1. **Machine Learning Engineer**: Design and develop AI models, algorithms, and systems that can learn from data.\n",
      "2. **Data Scientist**: Extract insights from large datasets to inform business decisions, using AI and machine learning techniques.\n",
      "3. **AI Researcher**: Conduct research in AI and machine learning, exploring new applications and improving existing technologies.\n",
      "4. **Natural Language Processing (NLP) Engineer**: Develop AI-powered language processing systems for applications like chatbots, voice assistants, and text analysis.\n",
      "5. **Computer Vision Engineer**: Design and develop AI-powered computer vision systems for applications like image recognition, object detection, and facial recognition.\n",
      "6. **Robotics Engineer**: Develop AI-powered robots that can perform tasks autonomously, using sensors, cameras, and other technologies.\n",
      "7. **AI Ethics Specialist**: Ensure AI systems are developed and used responsibly, considering ethical implications and potential biases.\n",
      "8. **AI Developer**: Build and deploy AI-powered applications, such as predictive maintenance, supply chain optimization, and customer service chatbots.\n",
      "9. **AI Consultant**: Help organizations implement AI solutions, providing strategic guidance and technical expertise.\n",
      "10. **AI Research Scientist**: Conduct research in AI and machine learning, publishing papers and presenting at conferences.\n",
      "11. **Autonomous Systems Engineer**: Design and develop autonomous systems, such as self-driving cars, drones, and robots.\n",
      "12. **AI Product Manager**: Oversee the development and launch of AI-powered products, ensuring they meet customer needs and business goals.\n",
      "13. **AI Quality Assurance Engineer**: Test and ensure the quality of AI-powered systems, identifying and addressing defects.\n",
      "14. **AI Technical Writer**: Create documentation and training materials for AI-powered systems, helping users understand and utilize the technology.\n",
      "15. **AI Sales Representative**: Sell AI-powered solutions to organizations, highlighting the benefits and value they can bring.\n",
      "\n",
      "These are just a few examples of the many career paths available in AI. Remember to stay up-to-date with industry developments, as new roles and specializations emerge as AI continues to evolve.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \" You will be acting as a career coach. Your goal is to give career advice to users. For questions that are not career related, don't provide advice. Say, I don't know.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory() # reset history\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"career_chat_history\",\n",
    ")\n",
    "\n",
    "response=wrapped_chain.invoke({\"input\": \"What are the career options in AI?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. As a career coach, I'm not qualified to provide advice on car repairs. I recommend taking your car to a trusted mechanic or a repair shop for assistance.\n"
     ]
    }
   ],
   "source": [
    "response=wrapped_chain.invoke({\"input\": \"How to fix my car?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What are the career options in AI?\n",
      "AI: What an exciting field! Artificial Intelligence (AI) has opened up numerous career opportunities across various industries. Here are some of the most in-demand roles:\n",
      "\n",
      "1. **Machine Learning Engineer**: Design and develop AI models, algorithms, and systems that can learn from data.\n",
      "2. **Data Scientist**: Extract insights from large datasets to inform business decisions, using AI and machine learning techniques.\n",
      "3. **AI Researcher**: Conduct research in AI and machine learning, exploring new applications and improving existing technologies.\n",
      "4. **Natural Language Processing (NLP) Engineer**: Develop AI-powered language processing systems for applications like chatbots, voice assistants, and text analysis.\n",
      "5. **Computer Vision Engineer**: Design and develop AI-powered computer vision systems for applications like image recognition, object detection, and facial recognition.\n",
      "6. **Robotics Engineer**: Develop AI-powered robots that can perform tasks autonomously, using sensors, cameras, and other technologies.\n",
      "7. **AI Ethics Specialist**: Ensure AI systems are developed and used responsibly, considering ethical implications and potential biases.\n",
      "8. **AI Developer**: Build and deploy AI-powered applications, such as predictive maintenance, supply chain optimization, and customer service chatbots.\n",
      "9. **AI Consultant**: Help organizations implement AI solutions, providing strategic guidance and technical expertise.\n",
      "10. **AI Research Scientist**: Conduct research in AI and machine learning, publishing papers and presenting at conferences.\n",
      "11. **Autonomous Systems Engineer**: Design and develop autonomous systems, such as self-driving cars, drones, and robots.\n",
      "12. **AI Product Manager**: Oversee the development and launch of AI-powered products, ensuring they meet customer needs and business goals.\n",
      "13. **AI Quality Assurance Engineer**: Test and ensure the quality of AI-powered systems, identifying and addressing defects.\n",
      "14. **AI Technical Writer**: Create documentation and training materials for AI-powered systems, helping users understand and utilize the technology.\n",
      "15. **AI Sales Representative**: Sell AI-powered solutions to organizations, highlighting the benefits and value they can bring.\n",
      "\n",
      "These are just a few examples of the many career paths available in AI. Remember to stay up-to-date with industry developments, as new roles and specializations emerge as AI continues to evolve.\n",
      "Human: How to fix my car?\n",
      "AI: I don't know. As a career coach, I'm not qualified to provide advice on car repairs. I recommend taking your car to a trusted mechanic or a repair shop for assistance.\n"
     ]
    }
   ],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ask a question that is not within this persona's specialty. The model should not answer that question and should give a reason for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.5 Chatbot with Context\n",
    "\n",
    "In this task, you ask the chatbot to answer questions based on context that was passed to it. You take a CSV file and use the Titan embeddings model to create a vector representing that context. This vector is stored in Facebook AI Similarity Search (FAISS). When the chatbot is asked a question, you will pass this vector back to the chatbot and have it retrieve the answer using the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titan embeddings Model\n",
    "\n",
    "Embeddings represent words, phrases, or any other discrete items as vectors in a continuous vector space. This allows machine learning models to perform mathematical operations on these representations and capture semantic relationships between them.\n",
    "\n",
    "You use embeddings for the Retrieval-Augmented Generation (RAG) [document search capability](https://labelbox.com/blog/how-vector-similarity-search-works/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model configuration\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS as VectorStore\n",
    "\n",
    "In order to use embeddings for search, you need a store that can efficiently perform vector similarity searches. In this notebook, you use FAISS, which is an in-memory store. To permanently store vectors, you can use Knowledge Bases for Amazon Bedrock, pgVector, Pinecone, Weaviate, or Chroma.\n",
    "\n",
    "The LangChain VectorStore APIs are available [here](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents:loaded:size=153\n",
      "Documents:after split and chunking size=154\n",
      "vectorstore_faiss_aws:created=<langchain_community.vectorstores.faiss.FAISS object at 0x7f36f1e25a50>::\n"
     ]
    }
   ],
   "source": [
    "# vector store\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "loader = CSVLoader(\"../rag_data/Amazon_SageMaker_FAQs.csv\") # --- > 219 docs with 400 chars\n",
    "documents_aws = loader.load() #\n",
    "print(f\"documents:loaded:size={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Documents:after split and chunking size={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "try:\n",
    "    \n",
    "    vectorstore_faiss_aws = FAISS.from_documents(\n",
    "        documents=docs,\n",
    "        embedding = br_embeddings, \n",
    "        #**k_args\n",
    "    )\n",
    "\n",
    "    print(f\"vectorstore_faiss_aws:created={vectorstore_faiss_aws}::\")\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a quick low code test \n",
    "\n",
    "You can use a Wrapper class provided by LangChain to query the vector database store and return the relevant documents. This runs a QA Chain with all default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon SageMaker supports R. You can use R within SageMaker notebook instances, which include a preinstalled R kernel and the reticulate library.\n"
     ]
    }
   ],
   "source": [
    "chat_llm=ChatBedrock(\n",
    "    model_id=\"amazon.titan-text-premier-v1:0\" , \n",
    "    client=bedrock_client)\n",
    "# wrapper store faiss\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)\n",
    "print(wrapper_store_faiss.query(\"R in SageMaker\", llm=chat_llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot application\n",
    "\n",
    "For the chatbot, you need context management, history, vector stores, and many other components. You start by building a Retrieval Augmented Generation (RAG) chain that supports context.\n",
    "\n",
    "This uses the **create_stuff_documents_chain** and **create_retrieval_chain** functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and functions used for RAG\n",
    "\n",
    "- **Retriever:** You used `VectorStoreRetriever`, which is backed by a `VectorStore`. To retrieve text, there are two search types to choose from: `\"similarity\"` or `\"mmr\"`. `search_type=\"similarity\"` uses similarity search in the retriever object, where it selects text chunk vectors that are most similar to the question vector.\n",
    "\n",
    "- **create_stuff_documents_chain** specifies how retrieved context is fed into a prompt and LLM. Retrieved documents are \"stuffed\" as context without any summarization or other processing into the prompt.\n",
    "\n",
    "- **create_retrieval_chain** adds the retrieval step and propagates the retrieved context through the chain, providing it alongside the final answer. \n",
    "\n",
    "If the question asked falls outside the scope of the context, the model will reply that it doesn't know the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is sagemaker?', 'context': [Document(metadata={'source': '../rag_data/Amazon_SageMaker_FAQs.csv', 'row': 7}, page_content='\\ufeffWhat is Amazon SageMaker?: What if I have my own notebook, training, or hosting environment?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker provides a full end-to-end workflow, but you can continue to use your existing tools with SageMaker. You can easily transfer the results of each stage in and out of SageMaker as your business requirements dictate.'), Document(metadata={'source': '../rag_data/Amazon_SageMaker_FAQs.csv', 'row': 12}, page_content='\\ufeffWhat is Amazon SageMaker?: What is Amazon SageMaker Studio?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models. You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive. All ML development activities including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface.'), Document(metadata={'source': '../rag_data/Amazon_SageMaker_FAQs.csv', 'row': 92}, page_content='\\ufeffWhat is Amazon SageMaker?: What algorithms does Amazon SageMaker use to generate models?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker includes built-in algorithms for linear regression, logistic regression, k-means clustering, principal component analysis, factorization machines, neural topic modeling, latent dirichlet allocation, gradient boosted trees, sequence2sequence, time-series forecasting, word2vec, and image classification. SageMaker also provides optimized Apache MXNet, Tensorflow, Chainer, PyTorch, Gluon, Keras, Horovod, Scikit-learn, and Deep Graph Library containers. In addition, Amazon SageMaker supports your custom training algorithms provided through a Docker image adhering to the documented specification.'), Document(metadata={'source': '../rag_data/Amazon_SageMaker_FAQs.csv', 'row': 44}, page_content='\\ufeffWhat is Amazon SageMaker?: How does Amazon SageMaker Data Wrangler work with my CI/CD processes?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Once you have prepared your data, Amazon SageMaker Data Wrangler provides different options for promoting your SageMaker Data Wrangler flow to production and integrates seamlessly with MLOps and CI/CD capabilities. You can configure and launch SageMaker processing jobs directly from the SageMaker Data Wrangler UI, including scheduling your data processing job and parametrizing your data sources to easily transform new batches of data at scale. Alternatively, SageMaker Data Wrangler integrates seamlessly with SageMaker processing and the SageMaker Spark container, allowing you to easily use SageMaker SDKs to integrate SageMaker Data Wrangler into your production workflow.')], 'answer': 'Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever=vectorstore_faiss_aws.as_retriever()\n",
    "question_answer_chain = create_stuff_documents_chain(chat_llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What is sagemaker?\"})\n",
    "print(response) # shows the document chunks consulted to come up with the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start a chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1387a1c7837472fbcc7475557aeee83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatUX(rag_chain, retrievalChain=True)\n",
    "chat.start_chat()  # Only answers will be shown here, and not the citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You have utilized Titan LLM to create a conversational interface with following patterns:\n",
    "\n",
    "- Chatbot (Basic - without context)\n",
    "- Chatbot using prompt template(Langchain)\n",
    "- Chatbot with personas\n",
    "- Chatbot with context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself\n",
    "\n",
    "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
    "- Play with the token length to understand the latency and responsiveness of the service.\n",
    "- Apply different prompt engineering principles to get better outputs.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file and continue with **Task 5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
